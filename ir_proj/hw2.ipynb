{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": false,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZStr8Th4jBq"
      },
      "source": [
        "# Assignment 2: Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN0aEQUD4mwy"
      },
      "source": [
        "## General guidelines\n",
        "\n",
        "This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±\n",
        "\n",
        "**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Staff will allow your notebook to **execute from start to finish for no more than 90 seconds**, then manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. \n",
        "\n",
        "**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle. Before submitting, **make sure the notebook executes from start to finish in less than 90 seconds**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FLuP8v74n4i"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "In this assignment, we are going to build an inverted index, empirically examine the resulting word frequencies (Zipf's law) in a subset of the Wikipedia corpus, and test the impact of different indexing decisions (stopword removal, stemming, etc.) on index size. \n",
        "\n",
        "To save you (and us) time, in this assignment we are providing you with pre-processed data. The file `part15_preprocessed.pkl`, which you should upload to your environment, contains 2,000 articles from the wikipedia dump, where we already removed WikiMedia markdown from the body of articles, parsed any outgoing links to other wikipedia articles, and matched links to target article ID's. The entire code for preprocessing is [here](https://gist.github.com/nirg/e3b6d3b47607b22bab2175378e70a09d) and while you do not need to run it in this assignment, it is useful for you to look closely at it so you will be able to process the wiki dump in future assignments and/or in the project.  \n",
        "\n",
        "**Your tasks in this assigment are:**\n",
        "\n",
        "1. (50 Points) Complete the implementation of an inverted index that supports:\n",
        "  1. Reading one posting list at a time from a posting file in disk (20 Points).\n",
        "  2. Merging partial indices built from subsets/batches of documents using the BSBI algorithm (30 Points).\n",
        "2. (40 Points) Calculate various word statistics and examine Zipf's law. In particular, you will be asked to:\n",
        "  1. Calculate word counts using the title, body and anchor text of each page (10 Points).\n",
        "  2. Filter words and apply stemming (10 Points). \n",
        "  3. Create a list of corpus stop-words (10 Points).\n",
        "  4. Calculate and plot Zipf's law (5 Points).\n",
        "  5. Create a list of corpus rare words (5 Points).\n",
        "3. (10 Points) Examine the impact of different indexing decisions (stopword removal, rare word removal, and stemming) on index size. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "53kCjteP7hY9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "38edb8f560b77fd63be1e779e66ed364",
          "grade": false,
          "grade_id": "cell-289632d061509568",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "373169be-6eca-49ae-adf2-6775261345d8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%load_ext google.colab.data_table\n",
        "import bz2\n",
        "from functools import partial\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "import heapq\n",
        "from itertools import islice, count, groupby\n",
        "from xml.etree import ElementTree\n",
        "import codecs\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "from time import time\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKIagN-f54oV"
      },
      "source": [
        "# Inverted index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9ndbxwo6tQk"
      },
      "source": [
        "## Helper classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0eOSE177yeq"
      },
      "source": [
        "Before we delve into the implementation of an Inverted Index, we'll define two classes that will help us read and write data that is partitioned into fixed-size files on disk. This way we don't have to worry about writing too long of a posting list to disk or too little to disk when implementing the inverted index class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cMpm3gYA71n7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7fada4b6f4465e8c396e7e7741ac5ecb",
          "grade": false,
          "grade_id": "cell-5ee8e7f0a7d3b40b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Let's start with a small block size of 30 bytes just to test things out. \n",
        "BLOCK_SIZE = 30\n",
        "\n",
        "class MultiFileWriter:\n",
        "    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "    def __init__(self, base_dir, name):\n",
        "        self._base_dir = Path(base_dir)\n",
        "        self._name = name\n",
        "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
        "                          for i in count())\n",
        "        self._f = next(self._file_gen)\n",
        "    \n",
        "    def write(self, b):\n",
        "      locs = []\n",
        "      while len(b) > 0:\n",
        "        pos = self._f.tell()\n",
        "        remaining = BLOCK_SIZE - pos\n",
        "        # if the current file is full, close and open a new one.\n",
        "        if remaining == 0:  \n",
        "          self._f.close()\n",
        "          self._f = next(self._file_gen)\n",
        "          pos, remaining = 0, BLOCK_SIZE\n",
        "        self._f.write(b[:remaining])\n",
        "        locs.append((self._f.name, pos))\n",
        "        b = b[remaining:]\n",
        "      return locs\n",
        "\n",
        "    def close(self):\n",
        "      self._f.close()\n",
        "\n",
        "class MultiFileReader:\n",
        "  \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n",
        "  def __init__(self):\n",
        "    self._open_files = {}\n",
        "\n",
        "  def read(self, locs, n_bytes):\n",
        "    b = []\n",
        "    for f_name, offset in locs:\n",
        "      if f_name not in self._open_files:\n",
        "        self._open_files[f_name] = open(f_name, 'rb')\n",
        "      f = self._open_files[f_name]\n",
        "      f.seek(offset)\n",
        "      n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
        "      b.append(f.read(n_read))\n",
        "      n_bytes -= n_read\n",
        "    return b''.join(b)\n",
        "  \n",
        "  def close(self):\n",
        "    for f in self._open_files.values():\n",
        "      f.close()\n",
        "\n",
        "  def __exit__(self, exc_type, exc_value, traceback):\n",
        "    self.close()\n",
        "    return False "
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH39_OIa6yCe"
      },
      "source": [
        "## Index implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWRzIyj47_1e"
      },
      "source": [
        "Below is our core implementation of an inverted index. The inverted index class supports adding documents to the index, keeping global statistics about the documents added (e.g. the document frequency per word), and writing the index to disk. **Pay close attention to the implmentation as you will need to extend it next**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "jN0adL9J8GzM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1d7d6931605ef64673ac09baf945aed",
          "grade": false,
          "grade_id": "cell-61e592553cc49197",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "from contextlib import closing\n",
        "\n",
        "TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this \n",
        "                     # many bytes.\n",
        "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
        "\n",
        "class InvertedIndex:  \n",
        "  def __init__(self, docs={}):\n",
        "    \"\"\" Initializes the inverted index and add documents to it (if provided).\n",
        "    Parameters:\n",
        "    -----------\n",
        "      docs: dict mapping doc_id to list of tokens\n",
        "    \"\"\"\n",
        "    # stores document frequency per term\n",
        "    self.df = Counter()\n",
        "    # stores total frequency per term\n",
        "    self.term_total = Counter()\n",
        "    # stores posting list per term while building the index (internally), \n",
        "    # otherwise too big to store in memory.\n",
        "    self._posting_list = defaultdict(list)\n",
        "    # mapping a term to posting file locations, which is a list of \n",
        "    # (file_name, offset) pairs. Since posting lists are big we are going to\n",
        "    # write them to disk and just save their location in this list. We are \n",
        "    # using the MultiFileWriter helper class to write fixed-size files and store\n",
        "    # for each term/posting list its list of locations. The offset represents \n",
        "    # the number of bytes from the beginning of the file where the posting list\n",
        "    # starts. \n",
        "    self.posting_locs = defaultdict(list)\n",
        "    \n",
        "    for doc_id, tokens in docs.items():\n",
        "      self.add_doc(doc_id, tokens)\n",
        "\n",
        "  def add_doc(self, doc_id, tokens):\n",
        "    \"\"\" Adds a document to the index with a given `doc_id` and tokens. It counts\n",
        "        the tf of tokens, then update the index (in memory, no storage \n",
        "        side-effects).\n",
        "    \"\"\"\n",
        "    w2cnt = Counter(tokens)\n",
        "    self.term_total.update(w2cnt)\n",
        "    for w, cnt in w2cnt.items():\n",
        "      self.df[w] = self.df.get(w, 0) + 1\n",
        "      self._posting_list[w].append((doc_id, cnt))\n",
        "\n",
        "  def write(self, base_dir, name):\n",
        "    \"\"\" Write the in-memory index to disk and populate the `posting_locs`\n",
        "        variables with information about file location and offset of posting\n",
        "        lists. Results in at least two files: \n",
        "        (1) posting files `name`XXX.bin containing the posting lists.\n",
        "        (2) `name`.pkl containing the global term stats (e.g. df).\n",
        "    \"\"\"\n",
        "    #### POSTINGS ####\n",
        "    self.posting_locs = defaultdict(list)\n",
        "    with closing(MultiFileWriter(base_dir, name)) as writer:\n",
        "      # iterate over posting lists in lexicographic order\n",
        "      for w in sorted(self._posting_list.keys()):\n",
        "        self._write_a_posting_list(w, writer, sort=True)\n",
        "    #### GLOBAL DICTIONARIES ####\n",
        "    self._write_globals(base_dir, name)\n",
        "\n",
        "  def _write_globals(self, base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
        "      pickle.dump(self, f)\n",
        "\n",
        "  def _write_a_posting_list(self, w, writer, sort=False):\n",
        "    # sort the posting list by doc_id\n",
        "    pl = self._posting_list[w]\n",
        "    if sort:\n",
        "      pl = sorted(pl, key=itemgetter(0))\n",
        "    # convert to bytes\n",
        "    b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
        "                  for doc_id, tf in pl])\n",
        "    # write to file(s)\n",
        "    locs = writer.write(b)\n",
        "    # save file locations to index\n",
        "    self.posting_locs[w].extend(locs) \n",
        "\n",
        "  def __getstate__(self):\n",
        "    \"\"\" Modify how the object is pickled by removing the internal posting lists\n",
        "        from the object's state dictionary. \n",
        "    \"\"\"\n",
        "    state = self.__dict__.copy()\n",
        "    del state['_posting_list']\n",
        "    return state\n",
        "\n",
        "  @staticmethod\n",
        "  def read_index(base_dir, name):\n",
        "    with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n",
        "      return pickle.load(f)\n",
        "\n",
        "  @staticmethod\n",
        "  def delete_index(base_dir, name):\n",
        "    path_globals = Path(base_dir) / f'{name}.pkl'\n",
        "    path_globals.unlink()\n",
        "    for p in Path(base_dir).rglob(f'{name}_*.bin'):\n",
        "      p.unlink()"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd8XW8hw8Hx2"
      },
      "source": [
        "## Reading a posting list from disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mBz-NAe8Tnu"
      },
      "source": [
        "**YOUR TASK (20 POINTS)**: Complete the implementation of `posting_lists_iter`, a generator function of the `InvertedIndex` class that reads one posting list at a time from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "mfsgcIC88Sx4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "70e30d93ff892dff70a399d78529e639",
          "grade": false,
          "grade_id": "cell-bb8e8652367b9a3c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# The following inheritance (hack) lets us add a method to a class defined in a\n",
        "# separate cell.\n",
        "class InvertedIndex(InvertedIndex):\n",
        "  \n",
        "  def posting_lists_iter(self):\n",
        "    \"\"\" A generator that reads one posting list from disk and yields \n",
        "        a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n",
        "    \"\"\"\n",
        "    with closing(MultiFileReader()) as reader:\n",
        "      for w, locs in self.posting_locs.items():\n",
        "        # read a certain number of bytes into variable b\n",
        "        b = reader.read(locs, self.df[w] * TUPLE_SIZE)\n",
        "        posting_list = []\n",
        "        # convert the bytes read into `b` to a proper posting list.\n",
        "        for i in range(int(len(b)/TUPLE_SIZE)):\n",
        "          tf = b[(i*TUPLE_SIZE)+TUPLE_SIZE-2:(i*TUPLE_SIZE)+TUPLE_SIZE]\n",
        "          file_id = b[(i*TUPLE_SIZE)+TUPLE_SIZE-4:(i*TUPLE_SIZE)+TUPLE_SIZE-2]\n",
        "          posting_list.append((int.from_bytes(file_id, 'big'), int.from_bytes(tf, 'big')))\n",
        "        '''\n",
        "        s = str(b)\n",
        "        s = s.replace(\"'\", \"\").replace(\"'\", \"\").replace(\"b\", \"\")\n",
        "        x = s.split('\\\\x')\n",
        "        x.remove('')\n",
        "        counter = 1\n",
        "        id = -1\n",
        "        place = -1\n",
        "        for i in x:\n",
        "          if i != '00':\n",
        "            if counter == 1:\n",
        "              try:\n",
        "                id = int(i)\n",
        "                counter +=1\n",
        "              except:\n",
        "                s = 0\n",
        "            elif counter == 2:\n",
        "              try:\n",
        "                posting_list.append((id,int(i)))\n",
        "                counter = 1\n",
        "              except:\n",
        "                s= 0\n",
        "                '''\n",
        "        yield w, posting_list"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "M4AGD4_XIkNL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aac098a6d7dc2178ae043e15d7c61788",
          "grade": true,
          "grade_id": "cell-a96235ba83d578cc",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# build tiny index and write to disk for testing purposes\n",
        "docs = {\n",
        "    1: ['dog', 'ate', 'a', 'dog'],\n",
        "    2: ['a', 'cat', 'ate', 'a', 'dog']\n",
        "}\n",
        "index = InvertedIndex(docs=docs)\n",
        "index.write('.', 'test')\n",
        "index2 = InvertedIndex.read_index('.', 'test')\n",
        "# read posting lists from disk\n",
        "words, pls = zip(*index2.posting_lists_iter())\n",
        "\n",
        "# test that terms are read in correct order\n",
        "assert words == ('a', 'ate', 'cat', 'dog')\n",
        "# test that posting lists are of correct length\n",
        "assert list(map(len, pls)) == [2, 2, 1, 2]\n",
        "# test that doc_id and tf are read properly\n",
        "assert pls == ([(1, 1), (2, 2)], [(1, 1), (2, 1)], [(2, 1)], [(1, 2), (2, 1)])\n",
        "# test that total tf of the word 'a' sums up correctly\n",
        "assert 3 == sum(map(itemgetter(1), pls[0]))"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AqFqXSM8csk"
      },
      "source": [
        "## Merging partial indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZfcTqK8lCu"
      },
      "source": [
        "When working with big data such as Wikipedia we want to parallelize the index creation on multiple machines, each of which processes a certain number of documents at a time, and then merge the resulting partial indices. In assignment #3, you will learn how to do this easily using the MapReduce framework. For now, you are only asked to implement the logic of merging an existing list of indices. Your implemention should generalize the 10 big blocks merge (MergeBlocks) we saw in class, which is also described in detail in the [textbook section about BSBI](https://nlp.stanford.edu/IR-book/html/htmledition/blocked-sort-based-indexing-1.html).\n",
        "\n",
        "**YOUR TASK (30 POINTS)**: Complete the implementation of `merge_indices` below, a function that merges the partial indices, and writes out a merged index. As always, the inverted index needs to be lexicographically ordered and with posting list sorted by doc_id. It is okay for the merge to load into memory all global dictionaries of the partial indices at once because they are usually small enough to fit in memory. However, posting lists/files are too big to fit in memory all at once, and **you are not allowed to load all posting list files into memory at once**. Specifically, you are required to load only a fixed number of posting lists into memory at any point in time (see implementation requirements below). \n",
        "\n",
        "The result of executing `merge_indices` should have two side-effects:\n",
        "1. The inverted index instance (self) should have correct global statistics. For example, the document frequency for each word should be an aggregation of DF's from the partial indices. \n",
        "2. The merged posting lists and global dictionaries are written out to disk.\n",
        "\n",
        "**Implementation requirements**:\n",
        "\n",
        "  - No sorting whatsoever. Partial indices are already sorted, just merge.\n",
        "  - No more than N posting lists are held in memory at any point in time during the merge, where N = len(names).\n",
        "  - The merged index needs to be lexicographically ordered and with posting lists sorted by doc_id. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "voIO9d098kuW",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "528b601229b668fc1ca8d654324ceb4f",
          "grade": false,
          "grade_id": "cell-6d63e5ad79f8e25f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# inheritance hack again to add a class method\n",
        "class InvertedIndex(InvertedIndex): \n",
        "  def merge_indices(self, base_dir, names, output_name):\n",
        "    \"\"\" A function that merges the (partial) indices built from subsets of \n",
        "        documents, and writes out the merged posting lists.\n",
        "    Parameters:\n",
        "    -----------\n",
        "        base_dir: str\n",
        "            Directory where partial indices reside.\n",
        "        names: list of str\n",
        "            A list of index names to merge.\n",
        "        output_name: str\n",
        "            The name of the merged index.\n",
        "    \"\"\"\n",
        "    indices = [InvertedIndex.read_index(base_dir, name) for name in names]\n",
        "    iters = [idx.posting_lists_iter() for idx in indices]\n",
        "    self.posting_locs = defaultdict(list)\n",
        "    #### POSTINGS: merge & write out ####\n",
        "    done = False\n",
        "    counter = 0\n",
        "    lst_been = []\n",
        "    iters_remain = []\n",
        "    lst_to_write = []\n",
        "    with closing(MultiFileWriter(base_dir, output_name)) as writer:\n",
        "      while done == False:\n",
        "        counter = 0\n",
        "        for i in iters:\n",
        "          if i not in iters_remain:\n",
        "              next_item = next(i,None)\n",
        "              if next_item is not None:\n",
        "                lst_been.append((next_item,i))\n",
        "              else:\n",
        "                counter += 1\n",
        "        if counter == len(names) and len(lst_been) == 0:\n",
        "          break\n",
        "        first = lst_been[0]\n",
        "        index = []\n",
        "        index.append(0)\n",
        "        for i in range(1,len(lst_been)):\n",
        "          if first[0][0] == lst_been[i][0][0]:\n",
        "            first[0][1].extend(lst_been[i][0][1])\n",
        "            index.append(i)\n",
        "            iters_remain = []\n",
        "          if first[0][0] > lst_been[i][0][0]:\n",
        "            iters_remain.append(first[1])\n",
        "            first = lst_been[i][0]\n",
        "            index = []\n",
        "            index.append(i)\n",
        "          if first[0][0] < lst_been[i][0][0]:\n",
        "            iters_remain.append(lst_been[i][1])\n",
        "        #write minimum\n",
        "        lst_to_write.append(first)\n",
        "        to_remove = []\n",
        "        for i in index:\n",
        "          to_remove.append(lst_been[i])\n",
        "        for i in to_remove:\n",
        "          lst_been.remove(i)\n",
        "      #### GLOBAL DICTIONARIES ####\n",
        "      lst_final = []\n",
        "      for i in lst_to_write:\n",
        "        sum = 0\n",
        "        c = 0\n",
        "        for j in i[0][1]:\n",
        "          sum += j[1]\n",
        "          c += 1\n",
        "        self.term_total[i[0][0]] = sum\n",
        "        self.df[i[0][0]] = c\n",
        "        self._posting_list[i[0][0]] = i[0][1]\n",
        "        self._write_a_posting_list(i[0][0],writer,True)\n",
        "\n",
        "    self._write_globals(base_dir, output_name)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bmyabotOI7MM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "018f5f53ae35f4d57260966731e05250",
          "grade": true,
          "grade_id": "cell-41ecf2e58cc893d4",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# build a second tiny index and write it to disk for testing purposes\n",
        "docs = {\n",
        "    4: ['dog', 'ate', 'a', 'dog'],\n",
        "    3: ['a', 'giraffe', 'ate', 'a', 'dog']\n",
        "}\n",
        "index = InvertedIndex(docs=docs)\n",
        "index.write('.', 'test2')\n",
        "# merge\n",
        "InvertedIndex().merge_indices('.', ['test', 'test2'], 'merged')\n",
        "\n",
        "# test the files were created successfully\n",
        "assert Path('./merged.pkl').exists()\n",
        "assert all([Path(f'./merged_00{i}.bin').exists() for i in range(3)])\n",
        "# load merged index\n",
        "index3 = InvertedIndex.read_index('.', 'merged')\n",
        "# test df \n",
        "assert index3.df == {'a': 4, 'ate': 4, 'cat': 1, 'dog': 4, 'giraffe': 1}\n",
        "# test term_total\n",
        "assert index3.term_total == {'a': 6, 'ate': 4, 'cat': 1, 'dog': 6, 'giraffe': 1}\n",
        "# test posting locations\n",
        "assert index3.posting_locs == \\\n",
        "  {'a': [('merged_000.bin', 0)],\n",
        "   'ate': [('merged_000.bin', 24), ('merged_001.bin', 0)],\n",
        "   'cat': [('merged_001.bin', 18)],\n",
        "   'dog': [('merged_001.bin', 24), ('merged_002.bin', 0)],\n",
        "   'giraffe': [('merged_002.bin', 18)]}"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hObsJ5xO9smj"
      },
      "source": [
        "Now that we're done testing our inverted index implementation, we'll switch to a decent index block size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cCcAcoHs9qNO",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d9037ba528d63c70c3457d0f61423f15",
          "grade": false,
          "grade_id": "cell-7cde4227943ebbbf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "BLOCK_SIZE = 1999998 # == 2MB-2bytes, a number that is divisible by TUPLE_SIZE."
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spwpxAUe9q9O"
      },
      "source": [
        "# Word stats and Zipf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRmROsABCdW_"
      },
      "source": [
        "Let's load the 2,000 pre-processed articles into memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "bAZEJ_SYYTr-",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5ff5b14cf8720f6e49a1b714af1de51e",
          "grade": false,
          "grade_id": "cell-1b6327496bab9e81",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24f841e-8efc-44f7-f099-205840552636"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "pkl_file = \"part15_preprocessed.pkl\"\n",
        "try:\n",
        "    if os.environ[\"assignment_2_data\"] is not None:\n",
        "      pkl_file = Path(os.environ[\"assignment_2_data\"])\n",
        "except:\n",
        "   Exception(\"Problem with one of the variables\")\n",
        "   \n",
        "print(pkl_file)\n",
        "assert os.path.exists(pkl_file), 'You must upload this file.'\n",
        "with open(pkl_file, 'rb') as f:\n",
        "  pages = pickle.load(f)  \n"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part15_preprocessed.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZFjbBVTPDR"
      },
      "source": [
        "Each page is a tuple in the form of (page_id, title, body, [(target_page_id, anchor_text), ...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTs7GWFrdWm8"
      },
      "source": [
        "### Word counts combining title, body and anchor text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij4H74GrXeDG"
      },
      "source": [
        "\n",
        "**YOUR TASK (10 Points)**: Complete the implementation of `count_words` to count word occurences in the text of articles' title, body, and anchor text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "E0IRW-fe38up",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fa5dab258ba877769e9fc4adc2d79149",
          "grade": false,
          "grade_id": "cell-56819215ca8e398c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){,24}\"\"\", re.UNICODE)\n",
        "def tokenize(text):\n",
        "  return [token.group() for token in RE_WORD.finditer(text.lower())]"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "tJza9mbHYwrz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "52b1c3110de6f4385a023ba9fc2db374",
          "grade": false,
          "grade_id": "cell-e2bfc67253608f07",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def count_words(pages):\n",
        "  \"\"\" Count words in the text of articles' title, body, and anchor text using \n",
        "      the above `tokenize` function. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  pages: list of tuples\n",
        "    Each tuple is a wiki article with id, title, body, and \n",
        "    [(target_article_id, anchor_text), ...]. \n",
        "  Returns:\n",
        "  --------\n",
        "  list of str\n",
        "    A list of tokens\n",
        "  \"\"\"\n",
        "  word_counts = Counter()\n",
        "  lst = []\n",
        "  for wiki_id, title, body, links in pages:\n",
        "    tokens = tokenize(title)\n",
        "    # tokenize body and anchor text and count\n",
        "    tokBody = tokenize(body)\n",
        "    for i in tokens:\n",
        "      lst.append(i)\n",
        "    for i in tokBody:\n",
        "      lst.append(i)\n",
        "  word_counts = Counter(lst)\n",
        "  return word_counts"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e4_rdIBJONUr",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "310b58fd853b80e0dfc636ef9d06989f",
          "grade": true,
          "grade_id": "cell-c206c7c311fc57a6",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "word_counts = count_words(pages)\n",
        "# check values of most common and least common words\n",
        "assert word_counts.most_common(1)[0] == ('the', 50795)\n",
        "assert word_counts.most_common()[-1][1] == 1\n",
        "# test the number of words in the 100 first articles in the corpus\n",
        "assert len(word_counts) == 72016\n",
        "# check that all words appear\n",
        "assert 'heart' in word_counts\n",
        "assert 'sanctuary' in word_counts\n",
        "assert 'pope' in word_counts"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJZy9sIgDbMF"
      },
      "source": [
        "## Filter words and apply stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms7KE2uregH9"
      },
      "source": [
        "**YOUR TASK (10 points):** Complete the implementation of `filter_tokens` below, a function that return the list of tokens excluding certain tokens and applying stemming as needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "6wppoD_-t1VW",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5c36a55e9781d8ceabd89e339b76f8ef",
          "grade": false,
          "grade_id": "cell-17be02c6971bec04",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "# Getting tokens from the text while removing punctuations.\n",
        "def filter_tokens(tokens, tokens2remove=None, use_stemming=False):\n",
        "  ''' The function takes a list of tokens, filters out `tokens2remove` and \n",
        "      stem the tokens using `stemmer`. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  tokens: list of str. \n",
        "    Input tokens.\n",
        "  tokens2remove: frozenset. \n",
        "    Tokens to remove (before stemming).\n",
        "  use_stemming: bool. \n",
        "    If true, apply stemmer.stem on tokens. \n",
        "  Returns:\n",
        "  --------\n",
        "  list of tokens from the text.\n",
        "  '''\n",
        "  d = 0\n",
        "  copy = tokens\n",
        "  tokens = []\n",
        "  temp = []\n",
        "  if tokens2remove is None:\n",
        "    d =0\n",
        "  else:\n",
        "    for i in copy:\n",
        "      if i not in tokens2remove:\n",
        "        tokens.append(i)\n",
        "    copy = tokens\n",
        "\n",
        "  if use_stemming:\n",
        "    for i in copy:\n",
        "      j = stemmer.stem(i)\n",
        "      temp.append(j)\n",
        "    copy = temp\n",
        "  return copy"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ud5csR1CkeTy",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "87ef1ea340f88d9660bac5f8eaafba7e",
          "grade": true,
          "grade_id": "cell-fa6224eba1cf132d",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## test text\n",
        "text = '''Oh, little prince! Bit by bit I came to understand the secrets of your sad little life... \n",
        "For a long time you had found your only entertainment in the quiet pleasure of looking at the sunset. \n",
        "I learned that new detail on the morning of the fourth day, when you said to me: \n",
        "\"I am very fond of sunsets. Come, let us go look at a sunset now\". \n",
        "\"But we must wait,\" I said. \n",
        "\"Wait? For what?\" \n",
        "\"For the sunset. We must wait until it is time.  \" \n",
        "At first you seemed to be very much surprised. And then you laughed to yourself. You said to me:\n",
        "\"I am always thinking that I am at home!\"'''\n",
        "\n",
        "tokens = tokenize(text)\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "\n",
        "# test the number of tokens created\n",
        "assert len(filter_tokens(tokens)) == 115\n",
        "assert len(filter_tokens(tokens, english_stopwords, False)) == 51\n",
        "assert len(set(filter_tokens(tokens, frozenset([]), True))) == 69"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KrYc4nqEC9S"
      },
      "source": [
        "## Building our first inverted index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYqf7thN-AL3"
      },
      "source": [
        "We are now ready to process the corpus and create an inverted index. Thinking ahead of building a search engine, we want to have separate indices for the titles of articles, body of articles, and the anchor text associated with articles. This will allow us later on to create a ranking function that assigns different weights to these different components. For example, it is common to rank higher documents that match a query word in the title than a match in the body text. Here is how we will do it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JSn1pGyo9aQU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1d699659ec5a5c4d2127cbea8ea18ffc",
          "grade": false,
          "grade_id": "cell-2123d8b4710edf3e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f3b764-1ea3-4757-ebf9-ae74925f6d5d"
      },
      "source": [
        "# create directories for the different indices \n",
        "!mkdir body_indices title_index anchor_index\n",
        "\n",
        "# default tokenizer keeping all words\n",
        "default_tokenizer = lambda text: tokenize(text)\n",
        "\n",
        "# define batch iterator\n",
        "def batch_iterator(it, batch_size=1000):\n",
        "  \"\"\" Generator that yields items in a batch. Yields the batch \n",
        "      index (0, 1, ..) and an iterable of items in each batch of `batch_size` \n",
        "      in length. \n",
        "  \"\"\"\n",
        "  for i, group in groupby(enumerate(it), \n",
        "                          lambda x: x[0] // batch_size):\n",
        "    _, batch = zip(*group)\n",
        "    yield i, batch\n",
        "\n",
        "def process_wiki(pages, index_name, tokenize_func=default_tokenizer):\n",
        "  \"\"\" Process wikipedia: tokenize article body, title, anchor text, and create\n",
        "      indices for them. Each index is named `index_name` and placed in a \n",
        "      directory under the current dir named 'body_indices', 'title_index' \n",
        "      and 'anchor_index', respectively. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  pages: list of tuples\n",
        "    Each tuple is a wiki article with id, title, body, and \n",
        "    [(target_article_id, anchor_text), ...]. \n",
        "  index_name: str\n",
        "    The name for the index.\n",
        "  tokenize_func: function str -> list of str\n",
        "    Tokenization function that takes text as input and return a list of \n",
        "    tokens.\n",
        "  Returns:\n",
        "  --------\n",
        "  Three inverted index objects\n",
        "    body_index, title_index, anchor_index.\n",
        "  \"\"\"\n",
        "  # create the index for titles\n",
        "  title_index = InvertedIndex()\n",
        "  # collect anchor text tokens for each target article by its id\n",
        "  id2anchor_text = defaultdict(list)\n",
        "\n",
        "  # iterate over batches of pages from the dump\n",
        "  body_index_names = []\n",
        "  for batch_idx, batch_pages in batch_iterator(pages):\n",
        "    ids, titles, bodies, links = zip(*batch_pages)\n",
        "    target_ids, anchor_texts = zip(*[wl for l in links for wl in l])\n",
        "    # tokenize\n",
        "    titles = map(tokenize_func, titles)\n",
        "    bodies = map(tokenize_func, bodies)\n",
        "    anchor_texts = map(tokenize_func, anchor_texts)\n",
        "    # create a separate index of articles body for article in this batch\n",
        "    body_index = InvertedIndex()\n",
        "    for id, title, body in zip(ids, titles, bodies):\n",
        "      title_index.add_doc(id, title)\n",
        "      body_index.add_doc(id, body)\n",
        "    for target_id, anchor_text in zip(target_ids, anchor_texts):\n",
        "      id2anchor_text[target_id].extend(anchor_text)\n",
        "    body_index.write('./body_indices', f'{index_name}_{batch_idx}')\n",
        "    body_index_names.append(f'{index_name}_{batch_idx}')\n",
        "  # merge body indices from the different batches into one index and delete \n",
        "  # the parts\n",
        "  body_index = InvertedIndex()\n",
        "  body_index.merge_indices('./body_indices', \n",
        "                           body_index_names, index_name)\n",
        "  for idx_name in body_index_names:\n",
        "    InvertedIndex.delete_index('./body_indices', idx_name)\n",
        "  title_index.write('./title_index', index_name)\n",
        "  # create index for anchor text\n",
        "  anchor_index = InvertedIndex()\n",
        "  for id, tokens in id2anchor_text.items():\n",
        "    anchor_index.add_doc(id, tokens)\n",
        "  anchor_index.write('./anchor_index', index_name)\n",
        "  return body_index, title_index, anchor_index"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory â€˜body_indicesâ€™: File exists\n",
            "mkdir: cannot create directory â€˜title_indexâ€™: File exists\n",
            "mkdir: cannot create directory â€˜anchor_indexâ€™: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK-yPROWw5_l"
      },
      "source": [
        "Let's create the actual index that includes all words (no filtering whatsoever):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "uWFh0JDenaUe",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7bd3ce5167f354e62426e5bb357dbb0f",
          "grade": false,
          "grade_id": "cell-f44beb006a3ec97f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "608003a1-8623-4aa6-930b-8edea43edda0"
      },
      "source": [
        "all_words_body, all_words_title, all_words_anchor = \\\n",
        "  process_wiki(pages, 'all_words')"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-2396e219ba20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_words_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words_anchor\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mprocess_wiki\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-230-4e43b8da0b0a>\u001b[0m in \u001b[0;36mprocess_wiki\u001b[0;34m(pages, index_name, tokenize_func)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mbody_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInvertedIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   body_index.merge_indices('./body_indices', \n\u001b[0;32m---> 65\u001b[0;31m                            body_index_names, index_name)\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbody_index_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mInvertedIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./body_indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-221-7c0294b7eb79>\u001b[0m in \u001b[0;36mmerge_indices\u001b[0;34m(self, base_dir, names, output_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_been\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlst_been\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL0qgh7p-6iM"
      },
      "source": [
        "Using the index (or indices) we can easily look at some corpus statistics. Let's look at the most frequent words in body, title, and anchor text of articles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr7ElsuA4o7_"
      },
      "source": [
        "top_n = 5\n",
        "top_words = pd.DataFrame(\n",
        "    {'body_top':   all_words_body.term_total.most_common(top_n),\n",
        "     'title_top':  all_words_title.term_total.most_common(top_n),\n",
        "     'anchor_top': all_words_anchor.term_total.most_common(top_n)}\n",
        ")\n",
        "top_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smrx6a6f7l-_"
      },
      "source": [
        "**Point to consider**: Why is the word 'of' more frequent than 'the' in titles and anchor texts while it isn't more frequent in the body of articles? How would you test this hypothesis *scientifically*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5IBI7L7BbbU"
      },
      "source": [
        "## Corpus stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VS6Mba9AdLl"
      },
      "source": [
        "In order to create a list of *corpus* stopwords, words that are not indicative of relevance, we will examine the top 100 words by their document frequency (df). We'll produce those lists excluding English stopwords and words that have fewer than three characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OVd-Vk_B3a3C",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8fa10d27cfcbd96b7cc67a2602083623",
          "grade": false,
          "grade_id": "cell-d102f38ab1379ff4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Looking at top words excluding stopwords and words with fewer than three \n",
        "# characters\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "def filtered_counter(counter):\n",
        "  \"\"\" created a new counter without stopwords and len(word)<3. \"\"\"\n",
        "  return  Counter(\n",
        "    {k:v for k,v in counter.items() \n",
        "     if len(k) > 2 and not k in english_stopwords}\n",
        "  )\n",
        "cnt_body = filtered_counter(all_words_body.df)\n",
        "cnt_title = filtered_counter(all_words_title.df)\n",
        "cnt_anchor = filtered_counter(all_words_anchor.df)\n",
        "top_n = 100\n",
        "top_words = pd.DataFrame(\n",
        "    {'body_top':   cnt_body.most_common(top_n),\n",
        "     'title_top':  cnt_title.most_common(top_n),\n",
        "     'anchor_top': cnt_anchor.most_common(top_n)}\n",
        ")\n",
        "top_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfLD-Ic_26rh"
      },
      "source": [
        "**YOUR TASK (10 Points):** Complete the implementation of `get_corpus_stopwords` below to create a list of corpus stopwords. To determine which words are corpus stopwords, look at the above table of 100 most common words in body, title, and anchor text. Include as corpus stopword any word that you believe will not be helpful for document retrieval and/or relevance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "fKW4cc7ygF49",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "81c9c309b036a657cce8389200eaebe0",
          "grade": false,
          "grade_id": "cell-c4cbf1d157a94114",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def get_corpus_stopwords():\n",
        "  \"\"\" Returns a list of corpus stopwords \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()\n",
        "corpus_stopwords = get_corpus_stopwords()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aMiLrVXa7YYC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4ead4d03a87026aa277a9ba23e646be1",
          "grade": true,
          "grade_id": "cell-ddf216804d9bce56",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# test corpus stopwords size:\n",
        "assert len(corpus_stopwords)>2\n",
        "assert len(corpus_stopwords)<20\n",
        "# hash stopword list\n",
        "corpus_stopwords_hashed = list(map(_hash, corpus_stopwords))\n",
        "# test words\n",
        "assert '3eb5e055b6' in corpus_stopwords_hashed, 'oops, you missed a word'\n",
        "assert 'c25d1b9329' in corpus_stopwords_hashed, 'oops, you missed a word'\n",
        "assert '98e786f146' not in corpus_stopwords, 'oops, you added a wrong word'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axS60u8qLD0I"
      },
      "source": [
        "**Additional points to consider:** \n",
        " \n",
        "*   Think of two advantages of using document frequecy (df) instead of word frequency in the corpus in identifying corpus stopwords.\n",
        "*   Should the word 'thumb' be part of the corpus stopwords list?  \n",
        "*   Should the word 'new' be part of the corpus stopwords list? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQ4VZMDJKMV"
      },
      "source": [
        "## Zipf's law "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nVepOXJjlMN"
      },
      "source": [
        "Let's examine Zipf's law in our corpus. \n",
        "\n",
        "**YOUR TASK (5 Points):** Complete the implementation of `create_zipfs_graph` to plot Zipf's law on a log-log scale. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "kUd92V9zqu5P",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "46b713615657fa824ee34ae866140c6e",
          "grade": false,
          "grade_id": "cell-16d99c98092f471f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def create_zipfs_graph(word_counts):\n",
        "  ''' Plot a log-log scale Zipf's law graph.\n",
        "  Parameters:\n",
        "  -----------\n",
        "  word_counts: Counter\n",
        "    A counter of term occurences in a corpus. \n",
        "  Returns:\n",
        "  --------\n",
        "  rank: list of int\n",
        "    Rank of words (x-axis of Zipf's law)\n",
        "  frequency: list of int\n",
        "    Count of word occurences (y-axis of Zipf's law)\n",
        "  '''\n",
        "  # Assign the appropriate values to the variables rank and frequency used by \n",
        "  # the ploting function. \n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()\n",
        "  plt.loglog(rank, frequency, '.')\n",
        "  return rank, frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xTrHIQBCJJs-",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ebfd43f6e2f2712d617663a8efded62f",
          "grade": true,
          "grade_id": "cell-4d165a8e1cf97d29",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Zipf's law for words in the corpus, excluding stopwords and words with fewer\n",
        "# than 3 characters.\n",
        "cnt_body = filtered_counter(all_words_body.term_total)\n",
        "rank, frequency = create_zipfs_graph(cnt_body)\n",
        "\n",
        "# test rank\n",
        "assert rank[0] == 1\n",
        "assert np.diff(rank).max() == 1\n",
        "assert np.diff(rank).min() == 1\n",
        "assert 70787 == len(rank)\n",
        "# test frequency\n",
        "assert 9266 == frequency[0]\n",
        "assert all([d<=0 for d in np.diff(frequency)])\n",
        "assert 52373 == sum(frequency[:40])\n",
        "assert 38421 == sum([1 for f in frequency if f==1])\n",
        "assert 10104 == sum([1 for f in frequency if f==2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBGzSawrKH9c"
      },
      "source": [
        "**Point to consider**: If we want to reduce index size, are we better off removing the few top words or the many infrequent words? What would be the implications for retrieval quality?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2FfPa-BO_jB"
      },
      "source": [
        "## Rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3w0d6M4IL7"
      },
      "source": [
        "In addition to removing English stopwords and corpus stopwords, we would like to remove rare words. We will consider as rare all words with a fewer than three occurences in the body of articles in corpus (total word frequency of 2 or less). \n",
        "\n",
        "**YOUR TASK (5 Points):** Complete the implementation of `get_rare_words` to return a list of corpus rare words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "nPzZ2G2N4Hiu",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1b5137c11629f48a00e216b2b1d7e0d0",
          "grade": false,
          "grade_id": "cell-f105166612ca708c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def get_rare_words():\n",
        "  \"\"\" Returns a list of corpus rare words, appearing fewer than three times in \n",
        "      the body of articles in the corpus.\n",
        "  \"\"\"\n",
        "  cnt_body = filtered_counter(all_words_body.term_total)\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1pCN2f2c83Pz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f777933ee6242859b6430e0555e82442",
          "grade": true,
          "grade_id": "cell-85d4651bdb1a86cb",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "corpus_rarewords = get_rare_words()\n",
        "assert len(corpus_rarewords) > 48000\n",
        "assert 'yaron' in corpus_rarewords\n",
        "assert 'year2020' in corpus_rarewords\n",
        "assert 'uni' in corpus_rarewords\n",
        "assert 'unforgivable' in corpus_rarewords\n",
        "assert 'Ã¸stfold' not in corpus_rarewords\n",
        "assert 'new' not in corpus_rarewords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f1BeXclCs4Y"
      },
      "source": [
        "**Point to consider:** What are the pros and cons of keeping or removing rare words from the index? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H4s3aveG54S"
      },
      "source": [
        "# Index size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-L0654k4xXN"
      },
      "source": [
        "Great! Now that we've explored the words that make up our corpus we can proceed with index creation. We were asked to build three indices:\n",
        "\n",
        "1. Excluding English and corpus stopwords.\n",
        "2. Excluding English and corpus stopwords, and removing rare words. \n",
        "3. Excluding English and corpus stopwords, removing rare words, and applying stemming.\n",
        "\n",
        "To that end, we will use the `process_wiki` function defined earlier and provide it with three different tokenization functions that will yield the index we need. We also change our tokenization regex to exclude words shorter than 3 characters long.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLn8JsYFH0zx"
      },
      "source": [
        "## Creating indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "C72CDXAQHlZb",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d22af9d51c207d6a7de2bf3a6c32bf50",
          "grade": false,
          "grade_id": "cell-d92ac7950e557030",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "stop = frozenset(corpus_stopwords).union(english_stopwords)\n",
        "stop_rare = stop.union(corpus_rarewords)\n",
        "\n",
        "def tokenize_no_stop(text):\n",
        "  return filter_tokens(tokenize(text), stop)\n",
        "def tokenize_no_stop_rare(text):\n",
        "  return filter_tokens(tokenize(text), stop_rare)\n",
        "def tokenize_no_stop_stem(text):\n",
        "  return filter_tokens(tokenize(text), stop_rare, True)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "_, _, _ = process_wiki(pages, 'stop_removed', tokenize_no_stop)\n",
        "_, _, _ = process_wiki(pages, 'stop_rare_removed', tokenize_no_stop_rare)\n",
        "_, _, _ = process_wiki(pages, 'with_stemming', tokenize_no_stop_stem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wGNgYpFXVO3"
      },
      "source": [
        "**Point to consider**: The above code iterates over the corpus three times. If we were to change the implementation of `process_wiki` so that it reads each document only once and adds its content to three indices, will the code execute three times faster? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiBbXJQPEz3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emc1o6xnllDo"
      },
      "source": [
        "## Calculating sizes\n",
        "\n",
        "In order to examine the implications of our different word inclusion/exclusion criteria on index size, we will calculate the index size on disk in MB for each of the four indices we created in this assignment. For brevity, we will only look at the body indices and focus their posting files (ignoring the global dictionary statistics) since normally the posting files are much larger than the global dictionary / word statistics. \n",
        "\n",
        "**YOUR TASK (10 Points):** Plot the indices size in a bar plot, where the X axis is the names of the indices and the Y axis is the size in MB. Make sure the indices are presented in a descending order based on the index size (from largest to smallest).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "TPCpyTsIn3ym",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f34c8c872f30e9f3e3b4a8b5a27cda05",
          "grade": false,
          "grade_id": "cell-7a678e437cb492c7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "def get_size(name):\n",
        "  ''' Return the size in MB of the posting files of the body index named `name`.\n",
        "  '''\n",
        "  return sum(p.stat().st_size for p in Path('./body_indices').rglob(f'{name}*.bin'))/1000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "M9ZJyMrr9nfq",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b67b69528f4a5b441ee724cd81b43c26",
          "grade": false,
          "grade_id": "cell-1c51df503b52b333",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def plot_index_sizes(index_names):\n",
        "  ''' Plot index size (MB) for each index given in `index_names`. Indices need\n",
        "      to be presented in descending order, from largest (left) to smallest \n",
        "      (right).\n",
        "  Parameters:\n",
        "  -----------\n",
        "  index_names: list of str\n",
        "    List of string index names, e.g. 'all_words'\n",
        "  Returns:\n",
        "  --------\n",
        "  index_names: list of str\n",
        "    List of string index names sorted in descending order of index sizes.\n",
        "  sizes: list of int\n",
        "    List of descending index sizes.\n",
        "  '''\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()\n",
        "  plt.bar(index_names, sizes)\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.show()\n",
        "  return index_names, sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_eCxfpGyb7hM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7ffdc4dd64e40b28cab7598dd1a9e84a",
          "grade": true,
          "grade_id": "cell-4d8a9dfc58013b81",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "index_names, sizes = plot_index_sizes(\n",
        "  [\"stop_removed\", \"stop_rare_removed\", \"with_stemming\", \"all_words\"]\n",
        ")\n",
        "assert all([s>0 for s in sizes])\n",
        "assert all([d<0 for d in np.diff(sizes)])\n",
        "assert list(map(_hash, index_names)) == \\\n",
        "  ['1b30d25775', '1ed1e86d00', 'dbdc047638', 'fb4af71be4']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY8WMZk37V3I"
      },
      "source": [
        "**Points to consider:** \n",
        "\n",
        "*   Can you explain the difference in index sizes? \n",
        "*   Which of the indices is the smallest and why?\n",
        "*   What is giving us more savings in terms of index size, removing high-frequency words or low-frequency words?\n",
        "*   When processing the entire English Wikipedia (and not just 2000 articles as we did here), do you expect the removal of rare words to have a greater effect on index size?\n",
        "*   Should we use stemming going forward? \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}